# -*- coding: utf-8 -*-
"""Untitled1 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z0ejJQZ-audfNqsd3nERbko5dKeVdJzM

**Import Libraries**
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import math
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from langdetect import detect
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from flask import Flask, request, jsonify


"""** *italicized text*Donload Libraries**"""



nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

"""**Load Data**"""

# Load the data
df = pd.read_csv("./data.csv", encoding='latin-1')

# Show the first few rows
print(df.head())

"""**Data** **Understanding**"""

print("Shape of dataset:", df.shape)

# Column names
print("Columns:", df.columns.tolist())

# Info about data types
print("\nData types and non-null counts:")
print(df.info())

"""**Missing Values**"""

# Count of missing values
missing_counts = df.isnull().sum()

# Percentage of missing values
missing_percent = (missing_counts / len(df)) * 100

print("Missing values:\n===============\n",missing_counts)
print("\nMissing percentages:\n", missing_percent)

"""**Checking Duplicates**"""

# Count duplicates
duplicate_count = df.duplicated().sum()
duplicate_percent = (duplicate_count / len(df)) * 100

print(f"Duplicate rows: {duplicate_count} with percentage of {round(duplicate_percent)}%\n")
dup_diff_label = df[df.duplicated(subset='Message', keep=False)]

print("Messages that appear more than once (could be different labels):\n")
print(dup_diff_label.sort_values('Message'))

"""**Displaying Graph about the Duplicates**"""

# Calculate counts
total_rows = len(df)
unique_rows = total_rows - duplicate_count  # this will be after dropping, so let's calculate using original

# Better: use the original before dropping duplicates
original_total = len(df)
duplicate_total = df.duplicated().sum()
unique_total = original_total - duplicate_total

# Prepare data for plot
labels = ['Unique Rows', 'Duplicate Rows']
values = [unique_total, duplicate_total]

# Plot
sns.set(style="whitegrid")
plt.figure(figsize=(6, 4))
sns.barplot(x=labels, y=values, palette='coolwarm')

# Add value labels
for i, v in enumerate(values):
    plt.text(i, v + 5, str(v), ha='center', fontweight='bold')

# Titles
plt.title("Original Data vs Duplicated Rows")
plt.ylabel("Number of Rows")
plt.xlabel("Row Type")
plt.tight_layout()
plt.show()

"""# New section"""



"""**Droping Duplicates**"""

# Drop the exact duplicate rows from your DataFrame
df = df.drop_duplicates()

# Reset index (optional but keeps things tidy)
df = df.reset_index(drop=True)

# Confirm changes
print("✅ Duplicates removed.")
print("New shape of the data:", df.shape)
print("Remaining duplicates:", df.duplicated().sum())

"""**Visualize Class Balance**"""

# Count spam vs ham
class_counts = df['Label'].value_counts()

# Bar chart
plt.figure(figsize=(6, 4))
sns.barplot(x=class_counts.index, y=class_counts.values, palette='Set2')

# Add text labels on bars
for i, count in enumerate(class_counts.values):
    plt.text(i, count + 100, str(count), ha='center', fontweight='bold')

# Titles
plt.title("Class Balance: Spam vs Ham")
plt.xlabel("Message Type")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

"""**Text Cleaning**"""

def clean_text(text):
    text = text.lower()  # lowercase
    text = re.sub(r'[^a-z\s]', '', text)  # remove punctuation and numbers
    return text

# Apply to the message column
df['clean_message'] = df['Message'].apply(clean_text)

# Preview cleaned text
print(df[['Message', 'clean_message']].head())



"""**text processing **"""

stop_words = set(stopwords.words('english'))

def tokenize_and_remove_stopwords(text):
    words = word_tokenize(text)  # split into words
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words

# Apply the function
df['tokens'] = df['clean_message'].apply(tokenize_and_remove_stopwords)

# Show before and after
print(df[['clean_message', 'tokens']].head())

# Join tokens back into text for vectorizer
df['final_text'] = df['tokens'].apply(lambda x: ' '.join(x))

# Create TF-IDF features
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['final_text'])  # X = features

# Target variable
y = df['Label'].map({'ham': 0, 'spam': 1})  # Convert to 0/1
print(y)



X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

svm_model = LinearSVC()
svm_model.fit(X_train, y_train)
svm_pred = svm_model.predict(X_test)

print("SVM Accuracy:", accuracy_score(y_test, svm_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, svm_pred))
print("\nClassification Report:\n", classification_report(y_test, svm_pred))

def predict_spam_somali_only(message):
    try:
        # Detect language
        lang = detect(message)
        if lang != 'so':  # 'so' = Somali
            return "❗ Only Somali language is allowed."

        # Clean message
        message = message.lower()
        message = re.sub(r'[^a-z\s]', '', message)

        # Tokenize and remove English stopwords (you can customize this)
        words = word_tokenize(message)
        filtered_words = [word for word in words if word not in stopwords.words('english')]

        # Reconstruct text
        final_text = ' '.join(filtered_words)

        # Vectorize
        vectorized_input = vectorizer.transform([final_text])

        # Predict
        prediction = svm_model.predict(vectorized_input)[0]

        return "Spam" if prediction == 1 else "Ham"

    except Exception as e:
        return f"Error: {str(e)}"
print(predict_spam_somali_only("hello how are you "))

app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    message = request.form.get('message')

    if message:
        prediction = predict_spam_somali_only(message)
        return jsonify({'prediction': prediction})
    else:
        return jsonify({'error': 'Message is missing'}), 400

if __name__ == '__main__':
    app.run(host='127.0.0.1', port=5000, debug=True)

